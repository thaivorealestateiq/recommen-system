{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-crfsPn83RHH"
      },
      "source": [
        "# The data preparation for training class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a-0LwT3t3MfN"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "import pandas  as pd\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import requests\n",
        "import sklearn\n",
        "import random\n",
        "\n",
        "class MovieLens(Dataset):\n",
        "    def __init__(self,\n",
        "                 df: pd.DataFrame,\n",
        "                 total_df: pd.DataFrame,\n",
        "                 ng_ratio:int\n",
        "                 )->None:\n",
        "        '''\n",
        "        :param df: training dataframe\n",
        "        :param total_df: the entire dataframe\n",
        "        :param ng_ratio: negative sampling ratio\n",
        "        '''\n",
        "        super(MovieLens, self).__init__()\n",
        "\n",
        "        self.df = df\n",
        "        self.total_df = total_df\n",
        "        self.ng_ratio = ng_ratio\n",
        "\n",
        "        # self._data_label_split()\n",
        "        self.users, self.items, self.labels = self._negative_sampling()\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        '''\n",
        "        get lenght of data\n",
        "        :return: len(data)\n",
        "        '''\n",
        "        return len(self.users)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        transform userId[index], item[inedx] to Tensor.\n",
        "        and return to Datalaoder object.\n",
        "        :param index: idex for dataset.\n",
        "        :return: user,item,rating\n",
        "        '''\n",
        "        return self.users[index], self.items[index], self.labels[index]\n",
        "\n",
        "\n",
        "    def _negative_sampling(self) :\n",
        "        '''\n",
        "        sampling one positive feedback per #(ng ratio) negative feedback\n",
        "        :return: list of user, list of item,list of target\n",
        "        '''\n",
        "        df = self.df\n",
        "        total_df = self.total_df\n",
        "        users, items, labels = [], [], []\n",
        "        user_item_set = set(zip(df['userId'], df['movieId']))\n",
        "        total_user_item_set = set(zip(total_df['userId'],total_df['movieId']))\n",
        "        all_movieIds = total_df['movieId'].unique()\n",
        "        # negative feedback dataset ratio\n",
        "        negative_ratio = self.ng_ratio\n",
        "        for u, i in user_item_set:\n",
        "            # positive instance\n",
        "            users.append(u)\n",
        "            items.append(i)\n",
        "            labels.append(1.0)\n",
        "\n",
        "            #visited check\n",
        "            visited=[]\n",
        "            visited.append(i)\n",
        "            # negative instance\n",
        "            for i in range(negative_ratio):\n",
        "                # first item random choice\n",
        "                negative_item = np.random.choice(all_movieIds)\n",
        " \n",
        "                # check if item and user has interaction, if true then set new value from random\n",
        "                while (u, negative_item) in total_user_item_set or negative_item in visited :\n",
        "                    negative_item = np.random.choice(all_movieIds)\n",
        "                users.append(u)\n",
        "                items.append(negative_item)\n",
        "                visited.append(negative_item)\n",
        "                labels.append(0.0)\n",
        "        print(f\"negative sampled data: {len(labels)}\")\n",
        "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLdS_BcYmozE"
      },
      "source": [
        "# The Multi-layered perceptron neural collaborative filtering (NCF) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uoamQX_bCVfH"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_users:int,\n",
        "                 num_items:int,\n",
        "                 num_factor:int=8,\n",
        "                 layer=None,\n",
        "                 use_pretrain: bool = False,\n",
        "                 use_NeuMF:bool = False,\n",
        "                 pretrained_MLP=None\n",
        "                 ):\n",
        "        super(MLP,self).__init__()\n",
        "\n",
        "        if layer is None:\n",
        "            layer = [64,32,16]\n",
        "\n",
        "        self.pretrained_MLP = pretrained_MLP\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.use_pretrain = use_pretrain\n",
        "        self.user_embedding = nn.Embedding(num_users,layer[0]//2)\n",
        "        self.item_embedding = nn.Embedding(num_items,layer[0]//2)\n",
        "        self.use_NeuMF = use_NeuMF\n",
        "        MLP_layers=[]\n",
        "\n",
        "        for idx,factor in enumerate(layer):\n",
        "            # ith MLP layer (layer[i],layer[i]//2) -> #(i+1)th MLP layer (layer[i+1],layer[i+1]//2)\n",
        "            # ex) (64,32) -> (32,16) -> (16,8)\n",
        "\n",
        "            MLP_layers.append(nn.Linear(factor, factor // 2))\n",
        "            MLP_layers.append(nn.ReLU())\n",
        "\n",
        "        # unpacking layers in to torch.nn.Sequential\n",
        "        self.MLP_model = nn.Sequential(*MLP_layers)\n",
        "\n",
        "        self.predict_layer =nn.Linear(num_factor, 1)\n",
        "        self.Sigmoid  = nn.Sigmoid()\n",
        "\n",
        "        if self.use_pretrain:\n",
        "            self._load_pretrained_model()\n",
        "        else:\n",
        "            self._init_weight()\n",
        "\n",
        "    def _init_weight(self):\n",
        "        if not self.use_pretrain:\n",
        "            nn.init.normal_(self.user_embedding.weight,std=1e-2)\n",
        "            nn.init.normal_(self.item_embedding.weight,std=1e-2)\n",
        "            for layer in self.MLP_model:\n",
        "                if isinstance(layer,nn.Linear):\n",
        "                    nn.init.xavier_uniform_(layer.weight)\n",
        "        if not self.use_NeuMF:\n",
        "            nn.init.normal_(self.predict_layer.weight,std=1e-2)\n",
        "\n",
        "    def _load_pretrained_model(self):\n",
        "        self.user_embedding.weight.data.copy_(\n",
        "            self.pretrained_MLP.user_embedding.weight)\n",
        "        self.item_embedding.weight.data.copy_(\n",
        "            self.pretrained_MLP.item_embedding.weight)\n",
        "        for layer, pretrained_layer in zip(self.MLP_model,self.pretrained_MLP.MLP_model):\n",
        "            if isinstance(layer,nn.Linear) and isinstance(pretrained_layer,nn.Linear):\n",
        "                layer.weight.data.copy_(pretrained_layer.weight)\n",
        "                layer.bias.data.copy_(pretrained_layer.bias)\n",
        "\n",
        "    def forward(self,user,item):\n",
        "        embed_user = self.user_embedding(user)\n",
        "        embed_item = self.item_embedding(item)\n",
        "        embed_input = torch.cat((embed_user,embed_item),dim=-1)\n",
        "        output = self.MLP_model(embed_input)\n",
        "\n",
        "        if not self.use_NeuMF:\n",
        "            output = self.predict_layer(output)\n",
        "            output = self.Sigmoid(output)\n",
        "            output = output.view(-1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __call__(self,*args):\n",
        "        return self.forward(*args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4sJE2Y-m3Dt"
      },
      "source": [
        "# Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z7WlbRA6FdEn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def hit(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\tindex = pred_items.index(gt_item)\n",
        "\t\treturn np.reciprocal(np.log2(index+2))\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def metrics(model, test_loader, top_k, device):\n",
        "\tHR, NDCG = [], []\n",
        "\n",
        "\tfor user, item, label in test_loader:\n",
        "\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\n",
        "\t\tpredictions = model(user, item)\n",
        "\t\t_, indices = torch.topk(predictions, top_k)\n",
        "\n",
        "\t\trecommends = torch.take(\n",
        "\t\t\t\titem, indices).cpu().numpy().tolist()\n",
        "\n",
        "\t\tgt_item = item[0].item()\n",
        "\t\tHR.append(hit(gt_item, recommends))\n",
        "\t\tNDCG.append(ndcg(gt_item, recommends))\n",
        "\n",
        "\treturn np.mean(HR), np.mean(NDCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbnvU8dem6kl"
      },
      "source": [
        "# Training class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hRvZO7M9FOyw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "class Train():\n",
        "    def __init__(self,model:torch.nn.Module\n",
        "                 ,optimizer:torch.optim,\n",
        "                 epochs:int,\n",
        "                 dataloader:torch.utils.data.dataloader,\n",
        "                 criterion:torch.nn,\n",
        "                 test_obj,\n",
        "                 device='cuda',\n",
        "                 print_cost=True):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.epochs = epochs\n",
        "        self.dataloader = dataloader\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        self.print_cost = print_cost\n",
        "        self.test = test_obj\n",
        "\n",
        "    def train(self):\n",
        "        model = self.model\n",
        "        optimizer = self.optimizer\n",
        "        total_epochs = self.epochs\n",
        "        dataloader = self.dataloader\n",
        "        criterion = self.criterion\n",
        "        total_batch = len(dataloader)\n",
        "        loss = []\n",
        "        device = self.device\n",
        "        test = self.test\n",
        "\n",
        "        for epochs in range(0,total_epochs):\n",
        "            #avg_cost = 0\n",
        "            for user,item,target in dataloader:\n",
        "                user,item,target=user.to(device),item.to(device),target.float().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                pred = model(user, item)\n",
        "                cost = criterion(pred,target)\n",
        "                cost.backward()\n",
        "                optimizer.step()\n",
        "                #avg_cost += cost.item() / total_batch\n",
        "            if self.print_cost:\n",
        "                #print(f'Epoch: {(epochs + 1):04}, {criterion._get_name()}= {avg_cost:.9f}')\n",
        "                HR, NDCG = metrics(model,test,10,device)\n",
        "                print(\"Epochs: {} HR: {:.3f}\\tNDCG: {:.3f}\".format(epochs, np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "            #loss.append(avg_cost)\n",
        "\n",
        "        if self.print_cost:\n",
        "            print('Learning finished')\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANxNODl-m_4M"
      },
      "source": [
        "# Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsbAWBlF6K_j",
        "outputId": "7dcbd697-738e-456a-ad06-aba96fde9779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n",
            "negative sampled data: 501130\n",
            "negative sampled data: 61000\n",
            "data loaded!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'device: {device}')\n",
        "\n",
        "# print GPU information\n",
        "if torch.cuda.is_available():\n",
        "    print('Current cuda device:', torch.cuda.current_device())\n",
        "    print('Count of using GPUs:', torch.cuda.device_count())\n",
        "\n",
        "\n",
        "# directory to save checkpoints\n",
        "pretrain_dir = 'models'\n",
        "if not os.path.isdir(pretrain_dir):\n",
        "    os.makedirs(pretrain_dir)\n",
        "\n",
        "# the train test, and total dataset\n",
        "train_dataframe = pd.read_csv(\"./content/train.csv\")\n",
        "total_dataframe = pd.read_csv(\"./content/entire_dataset.csv\")\n",
        "test_dataframe = pd.read_csv(\"./content/evaluation.csv\")\n",
        "\n",
        "\n",
        "# make torch.utils.data.Data object\n",
        "train_set = MovieLens(df=train_dataframe,total_df=total_dataframe,ng_ratio=4)\n",
        "test_set = MovieLens(df=test_dataframe,total_df=total_dataframe,ng_ratio=99)\n",
        "\n",
        "# get number of unique userID, unique  movieID\n",
        "max_num_users,max_num_items = total_dataframe['userId'].max()+1, total_dataframe['movieId'].max()+1\n",
        "\n",
        "print('data loaded!')\n",
        "\n",
        "# dataloader for train_dataset\n",
        "dataloader_train= DataLoader(dataset=train_set,\n",
        "                        batch_size=32,\n",
        "                        shuffle=True,\n",
        "                        num_workers=0,\n",
        "                        )\n",
        "\n",
        "# dataloader for test_dataset\n",
        "dataloader_test = DataLoader(dataset=test_set,\n",
        "                             batch_size=100,\n",
        "                             shuffle=False,\n",
        "                             num_workers=0,\n",
        "                             drop_last=True\n",
        "                             )\n",
        "\n",
        "\n",
        "model = MLP(num_users=max_num_users,\n",
        "                num_items=max_num_items,\n",
        "                use_NeuMF=False)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model.to(device)\n",
        "# objective function is log loss (Cross-entropy loss)\n",
        "criterion = torch.nn.BCELoss()\n",
        "save_model = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max num user 611\n",
            "max num item 193610\n"
          ]
        }
      ],
      "source": [
        "print('max num user', max_num_users)\n",
        "print('max num item', max_num_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSaVcPQ64juf"
      },
      "source": [
        "# The training and saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHvfzN2g4uTw",
        "outputId": "82ebfda0-bf9f-4440-da91-93a70b2a3b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 0 HR: 0.731\tNDCG: 0.461\n",
            "Epochs: 1 HR: 0.746\tNDCG: 0.480\n",
            "Epochs: 2 HR: 0.774\tNDCG: 0.542\n",
            "Epochs: 3 HR: 0.816\tNDCG: 0.559\n",
            "Epochs: 4 HR: 0.825\tNDCG: 0.569\n",
            "Epochs: 5 HR: 0.821\tNDCG: 0.566\n",
            "Epochs: 6 HR: 0.813\tNDCG: 0.556\n",
            "Epochs: 7 HR: 0.816\tNDCG: 0.555\n",
            "Epochs: 8 HR: 0.811\tNDCG: 0.559\n",
            "Epochs: 9 HR: 0.802\tNDCG: 0.555\n",
            "Learning finished\n",
            "training time:897.47438\n",
            "HR: 0.802\tNDCG: 0.555\n"
          ]
        }
      ],
      "source": [
        "train = Train(model=model,\n",
        "              optimizer=optimizer,\n",
        "              criterion=criterion,\n",
        "              epochs=10,\n",
        "              test_obj=dataloader_test,\n",
        "              dataloader=dataloader_train,\n",
        "              device=device,\n",
        "              print_cost=True,)\n",
        "# measuring time\n",
        "start = time.time()\n",
        "train.train()\n",
        "if save_model:\n",
        "    pretrain_model_dir = os.path.join(pretrain_dir,\"MLP\"+'.pth')\n",
        "    torch.save(model,pretrain_model_dir)\n",
        "end = time.time()\n",
        "print(f'training time:{end-start:.5f}')\n",
        "HR,NDCG = metrics(model,test_loader=dataloader_test,top_k=10,device=device)\n",
        "print(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJC13xVc4r-e"
      },
      "source": [
        "# Model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29cnkeDh7nB2",
        "outputId": "4f7203b6-6299-4adc-c602-19c5c258b6e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (user_embedding): Embedding(611, 32)\n",
              "  (item_embedding): Embedding(193610, 32)\n",
              "  (MLP_model): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (predict_layer): Linear(in_features=8, out_features=1, bias=True)\n",
              "  (Sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZZfJN-GYEqH"
      },
      "source": [
        "# Inference checking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9XcSWZvYgs5"
      },
      "source": [
        "## Getting inference with corresponding metrics from a test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv86mb4mYl7S",
        "outputId": "cebe5c70-cf49-4799-fafe-e2356e5e7d02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HR: 0.8016393442622951\n",
            "NDCG: 0.5552984354797468\n",
            "User: 346, Recommendations: ['Jaws (1975)', 'Fantasia (1940)', 'Police Academy (1984)', 'Afterglow (1997)', 'Risky Business (1983)', 'Little Big Man (1970)', 'Corporation, The (2003)', 'True Romance (1993)', 'On the Waterfront (1954)', 'All About Lily Chou-Chou (Riri Shushu no subete) (2001)']\n"
          ]
        }
      ],
      "source": [
        "def metrics_with_recommendations_with_titles(model, test_loader, top_k, total_dataframe, device):\n",
        "    \"\"\"\n",
        "    Function to return the recommendatations with metrics\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: The trained model checkpoints\n",
        "    test_loader: The test data loader\n",
        "    top_k: Total numbers of movies to recommend\n",
        "    total_dataframe: The total dataframe to map the id of the movie with title\n",
        "    device: According to availability CPU or a GPU\n",
        "\n",
        "    Returns:\n",
        "    ---------\n",
        "    HR: The Hit rate metrics\n",
        "    NDCG: The NDCG metrics\n",
        "    all_recommendations: The top_k recommended movies based on k\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    HR, NDCG, all_recommendations = [], [], []\n",
        "\n",
        "    for user, item, label in test_loader:\n",
        "        user = user.to(device)\n",
        "        item = item.to(device)\n",
        "\n",
        "        predictions = model(user, item)\n",
        "        _, indices = torch.topk(predictions, top_k)\n",
        "\n",
        "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
        "\n",
        "        gt_item = item[0].item()\n",
        "        HR.append(hit(gt_item, recommends))\n",
        "        NDCG.append(ndcg(gt_item, recommends))\n",
        "\n",
        "        # Get movie titles for the recommended movies\n",
        "        recommended_titles = [total_dataframe[total_dataframe['movieId'] == rec]['title'].values[0] for rec in recommends]\n",
        "\n",
        "        all_recommendations.append({\n",
        "            'user': user.item() if user.numel() == 1 else user.tolist(),\n",
        "            'ground_truth': total_dataframe[total_dataframe['movieId'] == gt_item]['title'].values[0],\n",
        "            'recommendations': recommended_titles\n",
        "        })\n",
        "\n",
        "    return np.mean(HR), np.mean(NDCG), all_recommendations\n",
        "\n",
        "HR, NDCG, all_recommendations = metrics_with_recommendations_with_titles(model, test_loader=dataloader_test, top_k=10, total_dataframe=total_dataframe, device=device)\n",
        "\n",
        "# Print HR, NDCG\n",
        "print(\"HR:\", HR)\n",
        "print(\"NDCG:\", NDCG)\n",
        "\n",
        "# Print individual recommendations with movie titles\n",
        "for rec in all_recommendations:\n",
        "    print(f\"User: {rec['user'][0]}, Recommendations: {rec['recommendations']}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c0XZdd3Go59"
      },
      "source": [
        "# Getting recommendations for an existing user of the system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR95-0qiDEQf",
        "outputId": "b14fd7f4-3df4-49ee-8ae1-3af240eec266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User ID 123 found in the test loader.\n",
            "User: 123, Recommendations: ['Finding Nemo (2003)', '12 Years a Slave (2013)', 'Fight Club (1999)', \"We're the Millers (2013)\", 'Hotel Transylvania (2012)', 'Idiocracy (2006)', 'The Godfather Trilogy: 1972-1990 (1992)', 'Assassination of Jesse James by the Coward Robert Ford, The (2007)', 'Fast & Furious 6 (Fast and the Furious 6, The) (2013)', 'Snowden (2016)']\n"
          ]
        }
      ],
      "source": [
        "def inference_for_single_user_by_id(model, user_id, top_k, total_dataframe, test_loader, device):\n",
        "    \"\"\"\n",
        "    Function to perform inference for a single user by user ID and return recommendations with metrics.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: The trained model checkpoints\n",
        "    user_id: The ID of the user for whom to make recommendations\n",
        "    top_k: Total numbers of movies to recommend\n",
        "    total_dataframe: The total dataframe to map the id of the movie with title\n",
        "    test_loader: The DataLoader for the test set\n",
        "    device: According to availability CPU or a GPU\n",
        "\n",
        "    Returns:\n",
        "    ---------\n",
        "    recommendations: Dictionary with user ID, ground truth, and recommended titles\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    for user, item, label in test_loader:\n",
        "     if user[0] == user_id:\n",
        "        user = user.to(device)\n",
        "        item = item.to(device)\n",
        "\n",
        "        print(f\"User ID {user_id} found in the test loader.\")\n",
        "        predictions = model(user, item)\n",
        "        _, indices = torch.topk(predictions, top_k)\n",
        "\n",
        "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
        "\n",
        "        # Get movie titles for the recommended movies\n",
        "        recommended_titles = [total_dataframe[total_dataframe['movieId'] == rec]['title'].values[0] for rec in recommends]\n",
        "\n",
        "        return {'user': user.item() if user.numel() == 1 else user.tolist(),\n",
        "                'recommendations': recommended_titles}\n",
        "\n",
        "\n",
        "\n",
        "# Example of how to use the inference_for_single_user_by_id function\n",
        "user_id_to_infer = 123  # Replace with the user ID you want to infer\n",
        "recommendations = inference_for_single_user_by_id(model, user_id_to_infer, top_k=10, total_dataframe=total_dataframe, test_loader=dataloader_test, device=device)\n",
        "\n",
        "# Print individual recommendations with movie titles\n",
        "print(f\"User: {recommendations['user'][0]}, Recommendations: {recommendations['recommendations']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
